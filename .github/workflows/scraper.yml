name: Daily Crypto Scraper

# 実行タイミング：
# 毎日午前7時（JST）      0 22 * * *
# 毎日午前9時（JST）      0 0 * * *
# 毎日午前10時（JST）     0 1 * * *
# 毎日午前11時（JST）     0 2 * * *
#
# ログは最大4000行まで表示可能（それ以上は切り捨て）
# 
on:
  schedule:
    - cron: '30 5 * * *'
  workflow_dispatch:       # 手動でも実行できるように

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      # 1. コードのチェックアウト（pushのためにtokenを渡すのが重要）
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # 2. Python 3.10 セットアップ
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3. Playwright と BeautifulSoup のインストール
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install playwright beautifulsoup4
          python -m playwright install chromium

      # 4. クローラースクリプト実行（標準出力ログも保存）
      - name: Run crawler
        run: python basic_crawler.py > output.log 2>&1

      # 5. Git の設定（ユーザー名とメールアドレス）
      - name: Set up Git config
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      # 6. 結果ファイルをコミットしてPush
      - name: Commit and push result
        run: |
          git add result.csv output.log
          git commit -m "Add result from scraper: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
