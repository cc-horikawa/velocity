name: Daily Crypto Scraper

# 実行タイミング：
# 毎日午前7時（JST）      0 22 * * *
# 毎日午前9時（JST）      0 0 * * *
# 毎日午前10時（JST）     0 1 * * *
# 毎日午前11時（JST）     0 2 * * *
#
# ログは最大4000行まで表示可能（それ以上は切り捨て）
# 
on:
  schedule:
    - cron: '45 0 * * *'
  workflow_dispatch:       # 手動でも実行できるように

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      # 1. コードを取得
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # 2. Pythonセットアップ
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 3. 依存ライブラリをインストール
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install playwright beautifulsoup4
          python -m playwright install chromium

      # 4. クローラー実行（URLを引数で渡す）
      - name: Run crawler with URL
        run: |
          python basic_crawler.py https://coinmarketcap.com/currencies/brilliantcrypto/historical-data/ > output.log 2>&1

      # 5. Git設定
      - name: Set up Git config
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      # 6. コミット＆Push（CSVとログ）
      - name: Commit and push result
        run: |
          git add result.csv
          git commit -m "Add result from scraper: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
